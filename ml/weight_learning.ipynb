{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weight Learning for Consensus Forecast Aggregator\n",
        "\n",
        "This notebook trains weights for different forecast sources based on historical performance.\n",
        "\n",
        "## Methodology\n",
        "\n",
        "1. Collect historical resolved events with final pre-resolution probabilities from each source\n",
        "2. Train weights using logistic regression or constrained least squares\n",
        "3. Evaluate using Brier score and log loss\n",
        "4. Apply learned weights to live events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import brier_score_loss, log_loss\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sqlalchemy import create_engine\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Database connection\n",
        "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql://user:password@localhost:5432/forecast_db\")\n",
        "engine = create_engine(DATABASE_URL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Historical Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load resolved events with their outcomes\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    e.id as event_id,\n",
        "    e.title,\n",
        "    e.outcome,\n",
        "    e.resolution_date,\n",
        "    s.name as source_name,\n",
        "    s.id as source_id,\n",
        "    f.probability,\n",
        "    f.timestamp\n",
        "FROM events e\n",
        "JOIN forecasts f ON e.id = f.event_id\n",
        "JOIN sources s ON f.source_id = s.id\n",
        "WHERE e.resolved = TRUE\n",
        "  AND f.timestamp <= e.resolution_date\n",
        "  AND f.timestamp >= e.resolution_date - INTERVAL '24 hours'\n",
        "ORDER BY e.id, s.id, f.timestamp DESC\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql(query, engine)\n",
        "print(f\"Loaded {len(df)} forecast records from {df['event_id'].nunique()} resolved events\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the latest forecast per source per event (final pre-resolution probabilities)\n",
        "df_latest = df.groupby(['event_id', 'source_name']).first().reset_index()\n",
        "\n",
        "# Pivot to have one row per event with columns for each source\n",
        "df_pivot = df_latest.pivot(index='event_id', columns='source_name', values='probability')\n",
        "\n",
        "# Get outcomes\n",
        "outcomes = df_latest.groupby('event_id')['outcome'].first()\n",
        "\n",
        "# Convert outcomes to binary (1 for YES, 0 for NO)\n",
        "y = (outcomes == 'YES').astype(int).values\n",
        "\n",
        "# Fill missing values with 0.5 (neutral probability)\n",
        "X = df_pivot.fillna(0.5).values\n",
        "\n",
        "source_names = df_pivot.columns.tolist()\n",
        "\n",
        "print(f\"Training on {len(X)} events with {len(source_names)} sources\")\n",
        "print(f\"Sources: {source_names}\")\n",
        "print(f\"Outcome distribution: {np.bincount(y)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression model\n",
        "lr = LogisticRegression(fit_intercept=False, max_iter=1000)\n",
        "lr.fit(X, y)\n",
        "\n",
        "# Get weights (coefficients)\n",
        "weights_lr = lr.coef_[0]\n",
        "\n",
        "# Normalize weights to sum to 1\n",
        "weights_lr = weights_lr / weights_lr.sum()\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr.predict_proba(X)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "brier_lr = brier_score_loss(y, y_pred_lr)\n",
        "log_loss_lr = log_loss(y, y_pred_lr)\n",
        "\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(f\"Weights: {dict(zip(source_names, weights_lr))}\")\n",
        "print(f\"Brier Score: {brier_lr:.4f}\")\n",
        "print(f\"Log Loss: {log_loss_lr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 2: Constrained Least Squares (Minimize Brier Score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def brier_score_objective(weights, X, y):\n",
        "    \"\"\"Objective function: Brier score\"\"\"\n",
        "    # Normalize weights\n",
        "    weights = weights / weights.sum()\n",
        "    # Calculate weighted average predictions\n",
        "    y_pred = X @ weights\n",
        "    # Brier score\n",
        "    return np.mean((y_pred - y) ** 2)\n",
        "\n",
        "# Initial weights (equal)\n",
        "initial_weights = np.ones(len(source_names)) / len(source_names)\n",
        "\n",
        "# Constraints: weights sum to 1, all non-negative\n",
        "constraints = [\n",
        "    {'type': 'eq', 'fun': lambda w: w.sum() - 1}\n",
        "]\n",
        "bounds = [(0, 1) for _ in range(len(source_names))]\n",
        "\n",
        "# Optimize\n",
        "result = minimize(\n",
        "    brier_score_objective,\n",
        "    initial_weights,\n",
        "    args=(X, y),\n",
        "    method='SLSQP',\n",
        "    bounds=bounds,\n",
        "    constraints=constraints\n",
        ")\n",
        "\n",
        "weights_cls = result.x / result.x.sum()\n",
        "y_pred_cls = X @ weights_cls\n",
        "\n",
        "brier_cls = brier_score_loss(y, y_pred_cls)\n",
        "log_loss_cls = log_loss(y, y_pred_cls)\n",
        "\n",
        "print(\"Constrained Least Squares Results:\")\n",
        "print(f\"Weights: {dict(zip(source_names, weights_cls))}\")\n",
        "print(f\"Brier Score: {brier_cls:.4f}\")\n",
        "print(f\"Log Loss: {log_loss_cls:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison = pd.DataFrame({\n",
        "    'Source': source_names,\n",
        "    'Logistic Regression': weights_lr,\n",
        "    'Constrained LS': weights_cls\n",
        "})\n",
        "\n",
        "print(\"Weight Comparison:\")\n",
        "print(comparison)\n",
        "\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(f\"Logistic Regression - Brier: {brier_lr:.4f}, Log Loss: {log_loss_lr:.4f}\")\n",
        "print(f\"Constrained LS - Brier: {brier_cls:.4f}, Log Loss: {log_loss_cls:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Logistic Regression weights\n",
        "axes[0].bar(source_names, weights_lr, color='steelblue')\n",
        "axes[0].set_title('Logistic Regression Weights')\n",
        "axes[0].set_ylabel('Weight')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Constrained LS weights\n",
        "axes[1].bar(source_names, weights_cls, color='coral')\n",
        "axes[1].set_title('Constrained Least Squares Weights')\n",
        "axes[1].set_ylabel('Weight')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update Database with Learned Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose the best method (lower Brier score)\n",
        "if brier_lr < brier_cls:\n",
        "    best_weights = weights_lr\n",
        "    method = 'logistic_regression'\n",
        "else:\n",
        "    best_weights = weights_cls\n",
        "    method = 'constrained_least_squares'\n",
        "\n",
        "print(f\"Using {method} weights\")\n",
        "\n",
        "# Update database\n",
        "from sqlalchemy import text\n",
        "\n",
        "with engine.connect() as conn:\n",
        "    for source_name, weight in zip(source_names, best_weights):\n",
        "        update_query = text(\"\"\"\n",
        "            UPDATE sources \n",
        "            SET weight = :weight \n",
        "            WHERE name = :source_name\n",
        "        \"\"\")\n",
        "        conn.execute(update_query, {\"weight\": float(weight), \"source_name\": source_name})\n",
        "        conn.commit()\n",
        "\n",
        "print(\"Weights updated in database!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation: Per-Source Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Brier score for each source individually\n",
        "source_performance = {}\n",
        "\n",
        "for i, source_name in enumerate(source_names):\n",
        "    source_probs = X[:, i]\n",
        "    brier = brier_score_loss(y, source_probs)\n",
        "    log_loss_val = log_loss(y, source_probs)\n",
        "    source_performance[source_name] = {\n",
        "        'brier_score': brier,\n",
        "        'log_loss': log_loss_val\n",
        "    }\n",
        "\n",
        "perf_df = pd.DataFrame(source_performance).T\n",
        "perf_df = perf_df.sort_values('brier_score')\n",
        "\n",
        "print(\"Per-Source Performance (lower is better):\")\n",
        "print(perf_df)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "perf_df['brier_score'].plot(kind='barh', ax=ax, color='steelblue')\n",
        "ax.set_xlabel('Brier Score')\n",
        "ax.set_title('Source Performance (Brier Score)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
